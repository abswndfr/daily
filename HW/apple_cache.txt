	Neso COA

      direct mapping	tag								tag bits + line bits + line offset bits
	  associative		fully							tag bits  			 + line offset bits		(tag bits = block bits)
	  set associative 	k way set associative			tag bits + set bits  + line offset bits		(line bits = way bits + set bits)

		. 1 way set associative = direct mapping
        . N way set associative = associative 
		
	  tag directory = tag bits x # cache lines			 
	  way = # cache lines per set
	 
   	  priority : RR, LRU, LIFO, optimal, MRU					26 ~ 27

	  cache coherency											29 ~ 31
		Bus watching or Snooping – generally used for bus-based SMP – Symmetric Multiprocessor System/multi-core systems
		Directory-based – Message-passing – may be used in all systems but typically in NUMA system and in large multi-core systems
		Shared cache – generally used in multi-core systems
	
	
	
		3. Classifications of Computer Architecture
			Von Neumann, Harvard, & Modified Havard) - Harvard + cache 			
			Flynn's taxonomy(classification) : SISD, SIMD, MISD, MIMD(multiprocessor)
				
		4. Introduction to Memory				
			cache, main memory(MM) & secondary memory(SM) 
			
		5. Memory Hierarchy & Interfacing 
			hierarchy : ith memory has access time T_i & hit ratio H_i (instructions in main memory / instruction exectued)
		    effective memory access time T_avg 
 			  P has access to M1, M2, & M3 in parallel : H_1 * T_1 + ((1-H_1) * H_2) *       T_2  + ((1-H_1) * (1-H_2)) *              T_3
			  P has access to M1, M2, & M3 in serial   : H_1 * T_1 + ((1-H_1) * H_2) * (T1 + T_2) + ((1-H_1) * (1-H_2)) * (T_1 + T_2 + T_3)
		
		6. Memory Interfacing – Solved PYQs
			T_cache = 30ns, T_main = 150ns, H_cache = 0.8 
			  T_avg_p = 0.8 * 30 + 0.2 *  150 		= 54ns
			  T_avg_s = 0.8 * 30 + 0.2 * (150 + 30) = 60ns
			T_cache = 5ns, T_cache + T_main = 50ns, H_cache =  0.8
			  T_avg s = 0.8 * 5 + 0.2 * 50 = 14ns
			  
		7. Introduction to Cache Memory
			L1, L2 - internal vs. L3 - shared
			
			cache hit / miss : cache vs. main memory						(cache latency : time to process cache hit/miss)
			tag directory(data structure) : tag not found -> cache miss		(miss latency  : time to find out cache miss and place it into cache)
		
			page fault / hit : main memory vs storage						(requested information is not found even in main memory -> page fault service time)

			locality : spatial vs temporal
			
		8. Direct Memory Mapping

			cache 			 : lines'										// smallest loadable unit
			main memory 	 : blocks'		1. line size = block size
			
			main memory 	 : frames*  
			secondary memory : pages*		2. frame / page : equally divided memory chunk in main / secondary memory (f.s = p.s)
		
			ex) MM size : 64W, MM block size : 4W -> # blocks = 16
			
			PA bits for 64W = 6 bits			   PA[5:4] : tag #	 
				PA[5:2]	: block # 				-> PA[3:2] : line #
				PA[1:0] : block/line offset
				
		9. Direct Memory Mapping – Solved Examples
		
		10.MM : 4GB, Cache : 1 MB, Block : 4KB, Word : 1B	

			1. PA bit split	 3         2           1         0         PA[31:12] block #, PA[19:0] cache size
							109876543210 98765432 109876543210 		-> PA[31:20] tag #, PA[19:12] line #, PA[11:0] line offset

			2. tag directory : keeps tag bits per cache line		(cache line 0 - tag X, cache line 1 - tag Y, and so on.)
							   # of tag directory entries = # of cache lines.  size of tag directory entry = # tag bits
							   size = 12 X 2^12 (PA[31:20] -> 12 bits per tag #, PA[11:0] -> 2^12 cache lines per block)

		   MM : 256MB, Cache : 512KB, Block : 256KB, Word : 1B	

			1. PA bit split	        2           1         0            PA[27:18] block #, PA[18:0] cache size
							 765432109 8 765432109876543210 		-> PA[27:19] tag #, PA[18] line #, PA[17:0] line offset

			2. tag directory : keeps tag bits per cache line		
							   # of tag directory entries = # of cache lines
							   size = 9 X 2^18 (PA[27:19] -> 9 bits per tag #, PA[17:0] -> 2^18 cache lines per block)

		   MM : 16GB, block : 16KB, # tag bits : 10, Word : 1B
			
			1. PA bit split	    3          2          1         0      PA[33:14] block #, PA[23:0] cache size
							 3210987654 3210987654 32109876543210 	-> PA[33:24] tag #, PA[23:14] line #, PA[13:0] line offset
			
			
		11. Direct Memory Mapping – Solved PYQs (Part 1)
		
			Q1. 1MB cache memory, block size 256B. T_cache = 3ns, H_cache = 0.94. T_MM_1st = 20ns, T_MM_rest = 5ns. Word = 64b. T_avg ? 
			
				PA[2:0] word
				PA[7:3] word offset within a block	: 2^5
				
				T_avg = H_cache * T_cache + (1-H_cache) * (T_cache + T_MM) 
				      = 0.94 x 3 + (1-0.94) x (3 + (20 x 1 + (2^5-1) x 5))  <= shouldn't 2nd term be divided by 2^5 ?
					  = 2.82 + 10.68 = 13.5ns
				
			
		14. Direct Memory Mapping – Hardware Implementation
		
			tag bits = n, cache lines = l -> n-bit comparator & n l x L MUXs : hit latency = T_MUX + T_n-bit comparator (T_MUX << T_n-bit comparator)
				Q. MM size = 2GB, cache size = 1MB, comparator delay = 8 x n nsec -> # tag bits = 11 and thus hit latency is 88 nsec.
		
			block # bits = b, line # bits = L : block no. x -> x % 2^L  ->  cache line number !!
			
										cache line # 
			 tag bit  +  cache line  		0
			 tag bit  +  cache line  		1	
			 tag bit  +  cache line  		2
				...
			 tag bit  +  cache line  		l-1
			 
			 
		15. Associative Mapping
			
			1. compulsary miss. a.k.a cold miss 			: upon 1st reference of a memory block 						-> increase line size for spatial locality
	3Cs		2. conflict miss a.k.a collision/interence miss : upon reference to the word that got evicted from cache	-> associative mapping
			3. capacity miss								: working set size > cache size
			4. coherence miss
			5. coverage miss
			6. system related miss
			

			0  1  2  3  4  5  6  7  8  9 10 11 12 ... 30 31  
			-----------------------------------------------
                        0  1  2  3  4  5  6  7  8     26 27		
            28 29 30 31'32'33'34'35'36'37'38'39 				' cold miss because 32 ~ 39 were never in cache before
                        0  1  2  3  4  5  6  7  8     26 27		
            28 29 30 31'32'33'34'35'36'37'38'39 				' conflic miss because 32~39 were evicted by 0 ~ 7		
			

			Associative mapping : any block can be assigned to any cache line.  
								  PA bits are split as tag bits and block or line offset. (entire block # bits are used as tags -> fully associative mapping)
								  
								  no clue on memory block # in cache -> one comparator per cache line (comparator size is block # bits)
								  
								  hit latency = T_n-bit comparator  + T_or gate
								  
			Q. MM size = 2GB, block size = 2KB, comparator delay = 15 x n nsec, delay of multi-input OR gate = 7 nsec
			
				 3         2          1         0  
				 09876543210987654321 09876543210		PA[30:11] block #, PA[10:0] cache offset
				
				Hit latency = 7 + 15 x 20
				
		17. Set Associative Mapping	 (ARM)		

     			0        1        2         3			main memory
              / | \	   / | \	/ | \ 	  / | \
             0  1  2  3  4  5  0  1  2 	 3  4  5		cache 		 
		     +-----+  +-----+  +-----+  +------+
		      set 0    set 1	set 0	  set 1			1 set = 3 lines : 3 way set associative
		
			ex. MM 128B, cache 32B, block 4B, word 1B, 2 way set associative
			
			PA[6:0] MM					PA[6:2] # MM blocks	
			PA[4:0]	cache				PA[4:2] # cache lines
			PA[1:0] block offset		PA[3:2] # sets				not 4:3 !!!!
										PA[6:4] tag bits

			direct mapping  : mapping happens in cache lines			PA[4:2]
			set associative : mapping happens with respect to the sets 	PA[3:2]

				MM block no % no of set -> set no : doesn't need to search entire cache, but just a set only.
				

			ex1. MM 256MB, cache 1MB, block 128B, 2 way set associative : 1 set = 2 lines
			
			  PA[27:0] MM				PA[27:7] # MM blocks
			  PA[19:0] cache			PA[19:7] # cache lines	: 2^13
			  PA[6:0]  block offset		PA[18:7] # sets
										PA[27:19] tag bits		: 9

			  tag directory size = # of cache lines X tag bits = 2^13 X 9	
			  # comparator = # way x tag bits = 2 x 9 bit comparator							
				
			ex2. MM size 4MB, block 64B, tag bits 10, 4 way set associative

			  PA[21:0] MM 				PA[21:6] # MM blocks
			  PA[13:0] cache			PA[13:6] # cache lines
			  PA[5:0]  block offset		PA[11:6] # sets
										PA[21:12] tag bits
		
			ex3. cache size 256KB, tag bits 8, 8 way set associative (1 set = 8 lines) : MM size ?
			
			  PA[22:0] MM				PA[]			  
			  PA[17:0] cache 			PA[17:] # cache lines
			  PA[]						PA[14:]	# sets
										PA[22:]	tag bits
				
				MM size = 2^23B = 8MB

		22. Cache Memory Mapping – A Comparative Study

			MM size 8GB, block size 2KB, cache size 4MB.
																			tag directory	# comparator								
				direct mapping			PA[32:22] + PA[21:11] + PA[10:0]	11 x 2^11			1 x 11 bit								
				associative 			PA[32:11]			  + PA[10:0]    22 x 2^11			2^11 x 22 bit (2^11 : # cache lines)	
				4 way set associative	PA[32:20] + PA[19:11] + PA[10:0]    13 x 2^11			4 x 13 bit	  (4 : # ways)				


		25. Cache Design - An Overview

			block placement			map MM block to cache line		: direct mapping, associative, set associative 
			block identification	search cache line				: 
			block replacement		evict upon cache miss 			: compulsary, conflic, -> replace & move to next level
			write strategy			update from cache to MM			: 
			
									placement			identification									 	replacement		
            direct mapping			block % # lines		line# & offset -> tag comparison				 no decision needed									
            associative 			anywhere						  	  tag comparison				 policy reduces misses/penalties
            set associative			block % # sets		set index      -> tag comparison within the set	 policy reduces misses/penalties

			replacement policies
				random
				FIFO & LIFO
				recency based
				frequency based
				optimal / belady's 

			write strategy
				write hit
					white through (if less write traffic) : reliable & helps in data recovery but high latency
					write back/deferred					  : faster but no recovery. 	dirty-bit. MM updated upon replacement

				write miss	
					write allocate		: first block brought to cache and then write (write back preferred)	
					no write allocate	: directly update to MM
					
		26. Cache Replacement Policies - RR, FIFO, LIFO, & Optimal		
				random (RR)					(initial ARM)	
				FIFO & LIFO
				recency based				MRU, LRU, Pseudo LRU	: age bits
				frequency based
				optimal / belady's 			used as cache efficiency measuring tool
				
		27. Cache Replacement Policies - MRU, LRU, Pseudo-LRU, & LFU
				MRU : for cyclic patterns 		1,2,3,4,5,1,2,3,4,1,2
				LRU : temporal locality		a linked list : LRU  ...........  MRU   

					age bits
					
							0,1,2,3,4,2,3,1,5,6,5 
						
						0	3 2 1 0(3)2 1 0(3)2 3
						1   0 3 2 1 0 0 0(3)2 1 1 
						2   1 0 3 2 1 3 2 1 0(3)2 
						3   2 1 0 3 2 1 3 2 1 0 0
				
				PLRU: approximate measures for replacement due to memory overhead to keep sequences of age bits 

					0->down, 1->up		0,1,2,3,4,5,6,7,2,1,9,8 
					
					   b0 b1_u b1_l b2_uu b2_ul b2_lu b2_ll
						1   1         1								0
						1   1         0								1
						1	0		  0     1						2
						1   0		  0 	0						3
						0	0	 1	  0     0     1 	  			4
						0	0	 1	  0     0     0 	  			5
						0	0	 0	  0     0     0 	1  			6
						0	0	 0	  0     0     0 	0  			7
						1	0	 0	  0     1     0 	0  			2
						1	1	 0	  0     1     0 	0  			1
						0	1	 1	  0     1     1 	0  			4 -> 9 
						1	0	 1	  0     0     1 	0  			3 -> 8
						
					can be also used to find MRU : just follow b0->b1->b2
					
				LFU : frequency count per each cache entry

		29. Cache Coherence Problem & Cache Coherency Protocols
			state : Modified, Shared, Invalid, Exclusive, Owned, Forward	
			
			MSI, MOSI, MOESI, MESI	
		
				https://www.geeksforgeeks.org/cache-coherence-protocols-in-multiprocessor-system/
		
			cache coherency schemes   : S/W based ways to avoid coherency issue (compiler and OS prevents coherence issue)
			cache coherency protocols : H/W based
				. snooping or bus based : snoop on the bus whenever broadcase over the shared bus
					.. write update		: all other cache controllers get updated when one promotes from shared to modified
					    ... write through
						... write back
					.. write invalidate : all other cache controllers get invalidated when one promotes from shared to modified
					    ... write through
						... write back

				. directory based 
					.. nodes are connected using a scalable interconnection network implementing P2P commnunication.
						cf) snooping uses broadcase over bus with which caches are connected	

		30. Snooping-based Cache Coherency Protocol (AMD & Intel, ARM)
		

					P1							P2						P3						P4						MM
																														var A 
			to update var A
			  compulsary miss
				read request on bus
										read broadcast message	read broadcast message	read broadcast message
										snoop on the shared bus snoop on the shared bus snoop on the shared bus
										ignore msg bcs no var A	ignore msg bcs no var A ignore msg bcs no var A
																													provide var A to P1
			cache line state : S
																	to update var A
																	  compulsary miss
																		read request on bus
			read broadcast message		read broadcast message							read broadcast message									
			snoop on the shared bus		snoop on the shared bus 						snoop on the shared bus									
			IGNORE msg b/c state is S	ignore msg b/c no var A							ignore msg b/c no var A		
																													provide var A to P3
																	cache line state : S		
																	
			---------------------------------------------------------- if write invalidate is used ----------------------------------------------------------------																					
			modify var A
			cache line state : M <- S			
			broadcast invalidation sig on bus
																	cache line state : I							update var A if write through (update upon replacement if WB)
																	invalidate local copy of A 			
																			
																	modify var A 
																	  coherence miss
																			
			---------------------------------------------------------- if write udpate is used ----------------------------------------------------------------
			modify var A
			cache line state : M <- S			
			broadcast update sig on bus
																	cache line state : S							update var A if write through (update upon replacement if WB)
																	modify var A 


		31. directory based																		
			for distributed system involving multiple nodes communicating in interconnected network
			
			snooping based is feasible w/ less # of processors b/c of bus contention(bottlenect)
		
					P1							P2						P3						P4						
					cache						cache					cache					cache
					directory                   directory               directory               directory
					MM							MM						MM						MM
		
		
		
		
		
